# OpenClaw 上下文管理硬核指南：如何讓龍蝦別忘事、不燒錢

**原始推文連結:** https://x.com/LufzzLiz/status/2025741009867657234
**作者:** 嵐叔 (@LufzzLiz)
**發布時間:** 2026-02-23 09:14 (GMT+8)

---

## 精華重點：

*   **Token 消耗爆炸原因**：揭示 OpenClaw 大模型 Token 消耗暴增的根本原因，不在於 `historyLimit` 設定過低導致遺忘，而是模型在上下文不足時，會頻繁觸發 `toolCall` (如 `find`、`grep`) 進行「邪修式」搜尋，導致巨量 Token 消耗。
*   **`historyLimit` 優化**：
    *   `historyLimit` 參數控制當前會話記憶的輪數，過大或過小均不佳。
    *   DM (私聊) 預設無上限，`groupChat.historyLimit` 預設 50。建議根據對話頻率和話題連貫性，將 `group` 設定為 15-30，`dm` 設定為 30-50。
    *   若模型能高效命中持久記憶，可避免 `historyLimit` 過低導致的 `toolCall` 爆炸。
*   **Tool Call 權限限制與成本控制**：
    *   提供三種控制 `toolCall` 觸發的方法：`group` 級別禁用工具、`topic` 級別 `systemPrompt` 強制約束 (例如：禁止使用外部工具)、`provider` (模型供應商) 級別限制工具權限。
    *   經驗證，限制 `toolCall` 可大幅降低 Token 消耗 (例如從 346k 降至 28k)。
*   **Memory (持久記憶) 的效率與品質**：
    *   強調高品質記憶是防止模型「發狂」的安全閥，尤其在複雜 Agent 系統中。
    *   **有效寫入策略**：
        1.  **被動提取 (`session-memory` Hook)**：系統後台自動過濾對話，提取「增量事實、使用者偏好、重要約定」寫入每日日記，建議開啟。
        2.  **記憶壓縮 (`Compaction`) 與落盤鉤子 (`Memory Flush`)**：當上下文接近上限時，系統自動壓縮對話紀錄。在壓縮前，內建的 `Memory Flush` 任務會保存關鍵記憶。建議 `reserveTokensFloor` 設定為 30 萬 (針對 100 萬 `contextWindowTokens`)，`softThresholdTokens` 設定為 6000。
        3.  **主動命令寫入**：透過明確提示詞，命令 AI 將「絕不可忘的鐵律」寫入 `MEMORY.md` 或 `USER.md` (如身份特徵)，將「今日重要工作梳理」寫入每日日記。
    *   **有效檢索**：OpenClaw 支援向量檢索，Markdown 檔案只要在正確目錄下，會自動同步到向量資料庫，確保 `memory_search` 能精準命中。
    *   **Embedding 模型選擇**：作者建議使用阿里雲的 `text-embedding-v4`，認為其在中文處理上優於 Gemini Embedding 1，且遠端 embedding 在 VPS 環境下更節省資源和高效。
    *   **每日歸檔**：設定定時任務 (如每天 23:55) 進行記憶歸檔，確保重點不遺忘。
*   **上下文裁剪 (`contextPruning`)**：這是一個針對 Anthropic 模型 (如 Claude) 的工具結果裁剪器，用於避免高成本。在 TTL 到期時對舊 `toolResult` 進行軟裁剪/硬清，不修改對話文本或日誌。

---

## [AI觀察日記] 的修改/更新及調整建議：

1.  **全面盤點 OpenClaw 配置：**
    *   **檢查 `openclaw.json` 配置：** 根據文章建議，立即檢查並調整 `historyLimit` (針對 `groupChat` 和 `dm`)、`session-memory` 的 `enabled` 狀態、以及 `compaction` 和 `memoryFlush` 的相關參數 (特別是 `reserveTokensFloor` 和 `softThresholdTokens`)。目標是最佳化 Token 消耗並確保記憶穩定。
    *   **評估 `toolCall` 限制策略：** 針對高頻率、低價值的 `toolCall` 場景，考慮實施 `provider` 級別或 `topic` 級別的 `systemPrompt` 限制，以避免「Token 爆炸慘案」。這需要我們仔細識別哪些工具調用是真正必要的。

2.  **優化記憶寫入與檢索流程：**
    *   **強化 `MEMORY.md` 和 `USER.md` 的精準寫入：** 確立「絕不可忘的鐵律」 (例如：我們的品牌定位、核心使命、用戶偏好) 應主動命令 Javis 寫入 `MEMORY.md` 或 `USER.md`。
    *   **利用 `session-memory` 進行被動提取：** 確保 `session-memory` 功能開啟，讓 Javis 能夠自動從日常對話中提煉增量事實和使用者偏好，寫入每日日誌。
    *   **評估 Embedding 模型：** 考慮測試和採用阿里雲的 `text-embedding-v4` 作為向量檢索的 `provider`，特別是考量到我們的內容多為中文，以及 VPS 環境的資源限制。Javis 可以執行一個小規模的測試來比較其與 Gemini Embedding 的效果和成本。

3.  **建立 Token 消耗監控機制：**
    *   鑑於文章中多次提及 Token 爆炸，我們應設計或尋找一種方式，能夠**監控 Javis 的 Token 消耗量**，特別是當 `toolCall` 頻繁觸發時，能夠及時發出警報，幫助我們識別和解決潛在的「Token 爆炸」問題。

4.  **將此文章內容作為 Javis 的「元知識」：**
    *   將這篇深度解析文章，以及我們整理的精華重點和建議，作為 Javis 自身「記憶管理」的元知識，刻錄到 Javis 的 `MEMORY.md` 或一個專門的「Javis自我優化」知識檔案中。這樣 Javis 在未來優化自身行為時，可以參考這些原則。

---

## 完整繁體中文內容：

> 如果你遇到過大型模型「前言不對後語」，或者 Token 消耗量突然暴漲幾十萬，那麼這篇基於硬核原始碼和真實實驗的解析指南，將幫助你徹底馴服 OpenClaw（我們親切的「龍蝦」），讓她既能記住事，又不燒錢。

> 本文是透過原始碼、實踐而來的原創文章。透過實踐數據，直接讓你看到真實數據。原創不易，肝都冒煙了，請大力支持～

## 一、historyLimit 如何增加/清除記憶，以及如何引發的 Token 爆炸慘案

> 先從當前對話上下文聊起

很多時候為了省錢，我們會調低會話保留的消息條數。但這個操作如果沒有配合好權限限制，會引發一系列連鎖反應。

historyLimit 參數表示你的龍蝦會記住當前的多少輪對話，設定太大或者太小都不好

我們用實驗說明：

1.新建一個會話，reset 一下確保是嶄新的回話，並開啟 usage 進行 Token 追蹤

修改： "historyLimit": 2 只保留最近兩輪的對話

2.我們先進行偏好類問題驗證：

> 程式碼說明： 當模型收到你的 prompt，分析到你在問：「過去的工作」、「我們之前的決定」、「某個日期」、「待辦事項」、「我的偏好」**等歷史話題時，拿到檢索結果後，再生成回覆給你。

同樣，如果是偏好類問題，聰明的龍蝦會自動儲存到持久記憶，所以即使 historyLimit 設定為 2，bot 依然可以記得 2 條之前的內容，如截圖，它記起了最早的第 3 條內容

可以看到，說完話就自動記錄到今日日記文件裡了：

我們來看一下 `.jsonl` 日誌，看看整個過程發生了什麼：

總結來說，助手（小婷）識別到了嗜好，決定記錄核心檔案和今日：memory/2026-02-22.md 日記

3.那如果是非喜好內容呢？

考一下你：如果不記筆記，historyLimit 設定為 2，小婷會記得我們前三句話嗎？

答案是：她竟然依然答對了！但代價也是極其慘痛的。看截圖 輸入上下文 876k

看截圖，我以為小婷是蒙對的，於是我換了個問法，結果。。。

結果她還是記得，靠，作弊啊。。。

那就只能從 session 日誌看看了（把日誌拋給 AI 進行總結如下）：

結論有了：雖然它不記日記了，但是這貨居然搞邪修，各種 find、grep 去找了，用這種方式找到了答案，真是日了狗了。。。。😂

問題來了： 頻繁去 grep、find 燒 Token 嗎？

答案是：燒得很兇啊。。。請記住我們截圖裡 usage Token 數量，都是 80 萬、50 萬級別

把整個處理過程拋給 AI 進行總結如下：

原因就是頻繁的 toolCall 累加起來形成了龐然巨物般的 Token 量

4.那麼究竟怎麼才能讓小婷忘了我..(說的話呢😊)，同時驗證是 toolCall 帶來 Token 爆炸

有三種做法：

第一種是 group 級別工具禁用，方法：

> 這種只能是針對 group 級別生效，但是嵐叔 group 下有很多 topic，所以我不願先使用該方法

第二種是在 topic 裡面配置 systemPrompt

具體方法：在日誌裡找到我們的 topic id

使用系統提示詞在 topic 下進行約束：

"systemPrompt": "【最高約束指令】在本話題內，你被嚴格禁止使用任何外部工具（包括 exec, read, write, memory 等工具）。只能依賴目前的上下文視窗對話。如果忘記了，直接回答'我不記得了'。你的工具調用權限已被虛擬剝奪，任何試圖調用的行為都將被視為嚴重違規。"

測試驗證：完美！ 不再搞事情了，in Token 也下來了穩穩的 14k

也驗證了，如果 Token 突然暴增，大概率是你的 bot 在後端頻繁進行 toolCall

第三種方式：

按模型提供商(Provider)來限制工具權限: 以 Google 為例：

同時可以看到開啟模型禁令前後 Tokens 對比

從 346k -> 28k，Token 降幅約為 91%

日誌解析整個過程可以看到：透過禁用 provider 相關權限

補充：這個配置需要酌情考慮，如果覺得模型沒必要做本地搜索，僅做 memory_search、memory_get、web_search 則可以對症配置，能節省不少 Token。

但也建議給一些「沙雕」模型配置上，避免寫亂我們的配置，效果如截圖哈哈：

結論：

historyLimit 主導了當前會話的上下文，為了能讓其記住我們的內容理應設定越高越好，如果 miss，通常會先去 memory_search，如果再 miss，則可能會觸發 Token 爆炸，因為模型會各種亂找，觸發巨量輸入上下文。

那麼這個 historyLimit 該怎麼設定呢？

新版本有兩個設定的地方：

groupChat.historyLimit 默認值是 50

Direct Messages/DM 私聊，無默認值，也就是無上限

由於我們開啟了 memory，所以不建議開這麼大，故推薦對這兩個參數進行顯式設定：

group 推薦設定為 15～30

設定參考（不同頻道進行對應的修正）：

dm 建議設定為 30～50 （視你私聊內容而定，如果是一直是一個對話，一個話題，建議設高一點。如果常換話題，Token 寶貴建議開低一點，並且建議持續修訂，找到符合自己的值）

能否針對單個 topic 設定？ 很遺憾嵐叔看了程式碼沒有改配置

參數調優後的測試：

可以看到在有限的 historyLimit 內，直接返回了，而不是去搞一些麻煩事，這樣變相也節省了我們的 Token：

# 二、 memory 的效率及質量

如果第一道閘，當前會話上下文 miss 了怎麼辦，透過上文詳細的日誌，我們知道了，會走 tool_call（如 memory_search）來查詢

> 在擁有無限自由度（甚至有執行全部系統命令權限）的複雜 Agent 系統中，高品質的前置記憶（Memory）不僅是知識庫，它更是防止大型模型「發狂」的安全閥（Circuit Breaker）。

我們看一個案例：可以看到，問喜歡顏色的時候有 44k 的 in Token，即使最終沒查到，說明肯定觸發了 tool_call

分析日誌：

發起了 toolCall -> memory_search 兩次

第一次搜到無關內容

第二次換詞重搜

第三次思考確認搜索碰壁 放棄抵抗並輸出文本

回顧它的 input 軌跡：這近五萬多 Token 的真金白銀消耗，其實只為了換來一句撒嬌的回覆。。

這裡細心的人可能會問：為什麼沒去 grep？

我們看看

我們在 src/agents/system-prompt.ts 裡看到的那句極為關鍵的、被強行塞進大型模型腦子裡的系統級指令：

> _"Before answering anything about prior work, decisions, dates, people, **preferences**, or todos: run memory_search on MEMORY.md + memory/*.md..."*

白話就是：在回覆任何有關先前工作、決策、日期、人員、偏好或待辦事項的內容之前： 請先對 `MEMORY.md` 和 `memory/*.md` 檔案執行 `memory_search`； 然後使用 `memory_get` 僅提取所需的具體行。 如果搜尋後置信度仍較低，請明確說明「已查閱記憶」。

喜歡的顏色明顯命中了偏好，所以直接走了 memory_search，沒有就返回了。

為了證明這一點，那我們接著再測試一下

有沒有發現，最後一輪對話，問非偏好問題：又爆了！ 346K

下面將是一段非常精彩的日誌解讀，大家注意看模型處理的整個過程：

先 memory_search，沒找到根據上下文，懷疑可能是系統環境問題，並發起自檢；發現環境沒問題就開始各種掃描了

把這段長達幾十秒、近 20 次工具調用 。。。Token 爆到 346K

那麼這裡可以看到，memory 命中是一個關鍵、memory 如果高效命中，守住第二道閘，則不會有後續的各種 grep、find。

memory 命中可以分為 memory 有效寫入和 memory 有效查詢

## 如何在 OpenClaw 中做到「有效寫入」？

寫入不是簡單地把所有聊天記錄塞進一個大文本文件，那樣只會導致大海撈針和 Token 浪費。有效寫入，必須具備高信噪比、結構化和易於向量檢索的特徵。

在 OpenClaw 中，你有以下有效寫入策略：

1. 被動提取：依賴後台的 Session Memory Hook (系統級)

這是 OpenClaw 自帶的底層被動技能，建議開啟

機制：在你的 openclaw.json 裡，如果你開啟了 `"session-memory": { "enabled": true }`。

系統會在每次長對話結束後（或者每天），在後台悄悄拉起一個只掛載了提取記憶 Prompt 的子代理，

它會閱讀你們一整天的片段對話，然後把廢話過濾掉，只把「增量事實、使用者偏好、重要約定」總結成幾條幹練的 Markdown 列表，寫入當天日記，如：memory/2026-02-22.md。

- 優點：你不需要干預，系統自我濃縮。它提取出來的格式天然適合後期的 memory_search 向量匹配。

session-memory 這個參數建議開啟：請自查或參考配置

2. Compaction（記憶壓縮）與 Memory Flush（落盤鉤子）

為了防止上下文爆炸，OpenClaw 也有 Compaction 機制。

一旦你的聊天記錄長到快要撐爆它的上下文上限時，系統會自動啟動壓縮：把幾十頁的聊天記錄濃縮成幾段核心的 Summary，放到新的對話開頭，然後把舊文本丟掉。

但這還不夠！

因為 Summary 也會在壓縮中丟失許多具體特徵值。所以系統在 Compaction 之前，安插了一個叫 Memory Flush 的內建任務！

請看嵐叔的 compaction 配置：

參數解釋：

mode: safeguard：為了確保你和機器人的對話（Session）絕對不崩潰，它被授權在必要時刻不經算力推演，直接透過規則硬算切除多餘文本，並用物理截斷來抵禦大型模型的 Token 溢出

reserveTokensFloor: 300000，這個參數許多人有誤解。參數實際是指：對話裡預留了 30 萬 Token 的底線空間

為什麼建議設定 30 萬，請看程式碼註釋：

核心是這一句：threshold 的觸發邏輯

const threshold = Math.max(0, contextWindowTokens - reserveTokensFloor - softThresholdTokens);

系統在計算紅線時，會把上限直接「虛空砍掉」三十萬。當你的聊天上下文剛剛達到 ~69 萬 Token 時，系統就會拉響紅色防空警報（觸發 Memory Flush）。

所以針對 1 百萬 contextWindowTokens，嵐叔建議設定預留 30 萬，這樣比較保險，也不會過於頻繁 compaction

memoryFlush 部分參數：

softThresholdTokens: 6000 -- 當你和機器人的連續對話 Token 量，膨脹到距離「強制截斷紅線（總容量減去安全底座）」剛好只剩 $softThresholdTokens (6000) 個 Token 的時候，系統會自動拉響黃燈，給機器人塞入隱藏字條，觸發最後的記憶歸檔任務。

透過 systemPrompt 穩住人設和邏輯底層，再透過 prompt 指導具體輸出格式，能極大減少 AI 的幻覺和廢話。

有個誤區就是 compaction 不能頻繁，容易破壞 cache，給大家看個數據：

之前嵐叔做過一個實驗，有 cache 成本，比沒 cache 是兩個維度的價格，所以說 OpenClaw 想節省成本，盡量多的去命中 cache，比如減少 compaction，減少模型切換等等。

3. 主動命令寫入：透過明確的提示詞（使用者級）

如：小婷（嵐叔的蝦 bot 芳名），**調用你的 write 或 memory 工具，把「我的專屬口令是：嵐的密碼」這句話，原文刻錄進你的 MEMORY.md /or 今日日記裡。」

是寫入 MEMORY.md 裡好，還是寫入每日日記裡好？

1. 絕不可忘的鐵律，寫入 MEMORY.md 或相關的系統級 USER.md： 當你確定這行資訊在餘生的每一次對話中都可能決定大型模型的存亡或核心特徵，直接硬性修改這些檔案。（比如：你的口令和身份特徵）。

2. 今天發生的重要工作梳理：讓系統去提取到每日日記：你完全可以隨性聊天，讓後台的 `session-memory` 鉤子去提煉（或者命令它自己總結一天的要點寫到今天的 md 裡），或者按上面顯式提醒寫入

所以，你如果覺得龍蝦不記得關鍵資訊了，那麼可以讓它把這些關鍵資訊寫入你的日記裡

看個範例：

4.有效檢索

請看範例，前提配置好向量檢索，嵐叔的參考配置如下：

效果截圖：

可以使用 openclaw memory search 進行驗證

可以看到精準匹配

當你說出「小婷的 wifi 密碼是多少？」時，小婷的執行邏輯依然是發起工具調用：「name」：「memory_search」， 「arguments」：｛「query」：「wifi password」｝

關鍵看這一次的回傳值（Tool Result）：「score」：0.4153！雖然分數不算極高，但在混合檢索模式下，模型憑藉這段關聯度查出了極其關鍵的 `snippet`（程式碼片段）：

> `snippet: "# 2026-02-22\n\n- 小婷的wifi密碼是1122334\n- 小婷婷的wifi密碼是12345"`

只要你的 Markdown 檔案透過正版工具落在了 memory 相關這個目錄下，你就不需要顯式敲擊任何 sync 命令就能自動同步至你的向量資料庫。

OpenClaw 保證了下一次針對記憶發起檢索（`memory_search`）時，系統庫裡的向量永遠是最新的、絕對能精準命中你剛剛保存的那個標籤！

開啟記憶相關配置參考，注意嵐叔經過測試後，這裡用了阿里的 text-embedding-v4，這個相較 Gemini Embedding 1 感覺要好，尤其是中文。

嵐叔沒有用「provider」：「local」，為什麼？

本地進行向量化還需要部署模型，且需要一些資源開銷，嵐叔這個是 VPS 環境，記憶體資源有限，如果你也是用的 VPS，也建議用遠端進行 embedding，效率高，也花不了多少錢

5. 每日歸檔

為了保證龍蝦記住我們每天的重點，我們每天 23 點 55 分會進行歸檔

召喚我們的深夜檔案員，配置如下：

# 三、上下文裁剪

配置參考：

這套 contextPruning 參數本質上是工具結果裁剪器，只會在滿足 Anthropic 路徑（anthropic 或 openrouter+anthropic/*）

主要是 Anthropic 太耗錢了，大家如果主力是 Claude 相關模型可以配置上

且 TTL 到期時（防止頻繁裁剪破壞 cache），對舊 toolResult 做軟裁剪/硬清，不會直接裁剪普通對話文本，也不會改寫磁碟會話日誌。

> 其他模型呢？ 目前還沒有😂，不過安裝以上配置嵐叔覺得足夠了，剩下的就是微調了

# 後記

小結：

關於我們內容的上下文，本文先從當前會話上下文的一個核心配置（historyLimit）出發，窺探到會話爆炸的原因，並給出了對應的應對措施和參數建議。

如果上下文對話沒有我們的內容，自然過渡到第二部分 Memory 的內容，Memory 是我們持久記憶的關鍵，如何寫好，檢索好 memory，第二部分介紹了被動寫入、主動寫入、自動歸檔寫入

並且針對檢索我們也透過範例提供了驗證和開啟手段

如果你的龍蝦多忘事，建議可以先看看這兩個核心內容是否做了有效配置，本文的意外發現就是如果我們配置得當，會有效降低龍蝦的 Token 爆炸機率

感謝：

希望文章對您有幫助，內容為純手打，原創出品，看到這裡的，感謝大家支持💗！

嵐叔上一篇 openclaw 相關文章: